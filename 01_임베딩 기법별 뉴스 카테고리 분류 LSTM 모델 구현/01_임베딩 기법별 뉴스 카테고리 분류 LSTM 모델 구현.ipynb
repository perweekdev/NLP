{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpET-UKqkqDa",
        "outputId": "51756c0d-8d42-48d6-efd0-954250b5ec5d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqCWs-vhsdN1",
        "outputId": "48817835-9e16-4d28-d74c-ffcb746b2fc5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 목차\n",
        "\n",
        "1. 프로젝트 개요  \n",
        " 1.1. 문제 정의 및 목표  \n",
        " 1.2. 데이터셋 및 평가 방법 설명  \n",
        "\n",
        "2. 환경 설정 및 데이터 준비  \n",
        " 2.1. 환경 설정  \n",
        " 2.2. 데이터 로드 및 전처리  \n",
        " 2.3. 데이터셋·라벨 구조 및 길이 분석  \n",
        "\n",
        "3. 임베딩 기반 텍스트 분류 모델 구현  \n",
        " 3.1. 데이터셋 클래스 및 데이터 로더 정의  \n",
        " 3.2. 임베딩 벡터(Word2Vec, FastText, GloVe) 구성  \n",
        " 3.3. LSTM 기반 분류 모델 구조 정의  \n",
        " 3.4. 학습 설정 및 모델 학습(임베딩별 실험 포함)  \n",
        "\n",
        "4. 성능 비교, 추가 실험 및 결론  \n",
        " 4.1. 모델별 성능 비교 분석  \n",
        " 4.2. 개선 방안 및 추가 실험 결과  \n",
        " 4.3. 결론 및 한계점·향후 과제  "
      ],
      "metadata": {
        "id": "T2LaHwKA1y6e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 프로젝트 개요"
      ],
      "metadata": {
        "id": "Gv7KqzXN2CSP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1. 문제 정의 및 목표\n",
        "\n",
        "- 이 프로젝트는 20 Newsgroups 텍스트 데이터셋을 활용하여, 각 뉴스 문서를 20개의 주제 카테고리 중 하나로 자동 분류하는 딥러닝 기반 텍스트 분류 모델을 구현하는 것을 목표로 한다.\n",
        "​\n",
        "\n",
        "- Word2Vec, FastText, GloVe와 같은 서로 다른 임베딩 기법을 적용하여 텍스트를 벡터화하고, 이를 LSTM 기반 분류 모델에 입력해 임베딩 방식에 따른 성능 차이를 비교·분석한다.\n",
        "​\n",
        "\n",
        "- 학습된 모델에 대해 Accuracy, Precision, Recall, F1-score 등의 지표를 계산하고, 전처리·모델 구조·하이퍼파라미터 관점에서 성능 개선 방안을 탐색한다."
      ],
      "metadata": {
        "id": "9rweZCF62GFN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2. 데이터셋 설명\n",
        "\n",
        "- 데이터셋: 20 Newsgroups  \n",
        "\\*로컬에 압축 형태로 저장된 20news-bydate-train, 20news-bydate-test 디렉터리 구조 사용​  \n",
        "[다운로드 링크](https://figshare.com/articles/dataset/20news-bydate_tar_gz/3829962?file=5975967)\n",
        "\n",
        "- 구성\n",
        "    - 문서 수: 총 18,846개 뉴스 문서\n",
        "    - 카테고리 수: 총 20개(정치, 스포츠, 컴퓨터, 종교 등 다양한 주제의 뉴스 그룹)\n",
        "    - 데이터 형식: 각 파일이 하나의 문서이며, 텍스트 내용과 헤더·푸터·인용문이 포함된 원시 뉴스 기사\n",
        "\n",
        "- 전처리 방식\n",
        "    - 원본 뉴스에서 헤더, 푸터, 인용 문장을 제거하여 fetch_20newsgroups(remove=('headers','footers','quotes'))와 유사한 형태로 정리한다.\n",
        "    - 이후 소문자 변환, 특수문자 제거, 불용어 제거 등의 정제 과정을 거쳐 모델이 학습하기 적합한 텍스트 형태를 만든다.\n",
        "\n",
        "- 평가 방법\n",
        "    - 기본적으로 테스트 세트에 대해 분류 정확도(Accuracy)를 계산한다.\n",
        "    - 필요 시 각 클래스별 Precision, Recall, F1-score를 추가로 측정하여 임베딩 방식 간 성능을 비교한다."
      ],
      "metadata": {
        "id": "HPKl7JsX2Raw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 환경 설정 및 데이터 준비"
      ],
      "metadata": {
        "id": "Lr9vu8cP4K40"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1. 환경 설정\n",
        "\n",
        "- 텍스트 전처리, 임베딩 학습, 딥러닝 모델 구현을 위해 `gensim`, `torch`, `nltk`, `sklearn`, `matplotlib`, `numpy` 등을 임포트한다.\n",
        "\n",
        "- NLTK 토크나이저와 불용어 사전을 다운로드하고, GPU 사용 가능 여부에 따라 연산 장치를 설정한다."
      ],
      "metadata": {
        "id": "0-isJNio4P0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec, FastText\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "\n",
        "# NLTK 리소스 다운로드\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdMi75hy4Wy_",
        "outputId": "34961b77-cb68-40cc-b0b4-65b913fa1e60"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgQA0Uv-g9PT",
        "outputId": "726eaf88-fdb8-4410-841b-817cae082df9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# GPU 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_af2Pk0cSnC"
      },
      "source": [
        "### 2.2. 로컬 데이터 로드 및 전처리\n",
        "\n",
        "- 원래는 fetch_20newsgroups(subset='all', remove=('headers','footers','quotes'))로 데이터를 바로 다운로드·전처리하려 했으나, 환경 이슈(403 에러 등)로 인해 이 방식 사용이 어려웠다.\n",
        "\n",
        "- 따라서 20 Newsgroups 압축 파일을 로컬로 다운로드한 뒤, 디렉터리 구조(20news-bydate-train, 20news-bydate-test)에서 직접 파일을 읽어들이는 방식으로 데이터를 로드한다.\n",
        "\n",
        "- 이때, strip_newsgroup_header, strip_newsgroup_quoting, strip_newsgroup_footer 함수를 정의하여 헤더/인용/푸터를 제거하고, fetch_20newsgroups(remove=...)와 최대한 비슷한 형태로 텍스트를 정리한다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # 압축 파일 드라이브 위치후 최초 1회 실행(런타임 초기화 무관)\n",
        "\n",
        "# import tarfile\n",
        "# import os\n",
        "\n",
        "# tar_path = \"/content/drive/MyDrive/data/20news-bydate.tar.gz\"\n",
        "\n",
        "# with tarfile.open(tar_path, \"r:gz\") as tar:\n",
        "#     tar.extractall(path=base_path)"
      ],
      "metadata": {
        "id": "PrYURprbvVoK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "base_path = \"/content/drive/MyDrive/data/20news-bydate\"\n",
        "\n",
        "def strip_newsgroup_header(text):\n",
        "    _before, _blankline, after = text.partition(\"\\n\\n\")\n",
        "    return after\n",
        "\n",
        "_QUOTE_RE = re.compile(\n",
        "    r\"(writes in|writes:|wrote:|says:|said:|^In article|^Quoted from|^\\||^>)\"\n",
        ")\n",
        "\n",
        "def strip_newsgroup_quoting(text):\n",
        "    good_lines = [\n",
        "        line for line in text.split(\"\\n\")\n",
        "        if not _QUOTE_RE.search(line)\n",
        "    ]\n",
        "    return \"\\n\".join(good_lines)\n",
        "\n",
        "def strip_newsgroup_footer(text):\n",
        "    lines = text.strip().split(\"\\n\")\n",
        "    for line_num in range(len(lines) - 1, -1, -1):\n",
        "        line = lines[line_num]\n",
        "        if line.strip().strip(\"-\") == \"\":\n",
        "            break\n",
        "    if line_num > 0:\n",
        "        return \"\\n\".join(lines[:line_num])\n",
        "    else:\n",
        "        return text\n",
        "\n",
        "def clean_like_sklearn(text):\n",
        "    text = strip_newsgroup_header(text)\n",
        "    text = strip_newsgroup_quoting(text)\n",
        "    text = strip_newsgroup_footer(text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "MaZNXvfO5X7m"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_20news_flat(base_path):\n",
        "    texts = []\n",
        "    labels = []\n",
        "    label_map = {}\n",
        "    label_id = 0\n",
        "\n",
        "    # train / test 디렉터리 모두 순회\n",
        "    for split in [\"20news-bydate-train\", \"20news-bydate-test\"]:\n",
        "        split_path = os.path.join(base_path, split)\n",
        "\n",
        "        for category in sorted(os.listdir(split_path)):\n",
        "            category_path = os.path.join(split_path, category)\n",
        "            if not os.path.isdir(category_path):\n",
        "                continue\n",
        "\n",
        "            if category not in label_map:\n",
        "                label_map[category] = label_id\n",
        "                label_id += 1\n",
        "\n",
        "            for fname in os.listdir(category_path):\n",
        "                file_path = os.path.join(category_path, fname)\n",
        "                try:\n",
        "                    with open(file_path, encoding=\"latin1\") as f:\n",
        "                        raw_text = f.read()\n",
        "                        # fetch_20newsgroups(remove=('headers','footers','quotes'))유사하게 처리\n",
        "                        text = clean_like_sklearn(raw_text)\n",
        "                        texts.append(text)\n",
        "                        labels.append(label_map[category])\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "    return texts, labels"
      ],
      "metadata": {
        "id": "EcCjKZw45ekX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts, labels = load_20news_flat(base_path)\n",
        "print(f\"Number of documents: {len(texts)}\")\n",
        "print(f\"Sample document: \\n{texts[0]}\")\n",
        "print(f\"Unique labels: {set(labels)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eCmmfwg5k9s",
        "outputId": "3380475c-6b62-4b6a-86c3-bb67aaa3ed1b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of documents: 18846\n",
            "Sample document: \n",
            "Why are only those people in favor of the system to blame.  If society\n",
            "accepts such a system, then each member of society is to blame when\n",
            "an innocent person gets executed.  Those that are not in favor should\n",
            "work to convince others.\n",
            "\n",
            "And, most members of our society have accepted the blame--they've considered\n",
            "the risk to be acceptable.  Similarly, every person who drives must accept\n",
            "the blame for fatal traffic accidents.  This is something that is surely\n",
            "going to happen when so many people are driving.  It is all a question of\n",
            "what risk is acceptable.  It is much more likely that an innocent person\n",
            "will be killed driving than it is that one will be executed.\n",
            "Unique labels: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 데이터 구성 요약\n",
        "\n",
        "  | 항목        | 내용                                                                                 |\n",
        "  | --------- | ---------------------------------------------------------------------------------- |\n",
        "  | 총 문서 수    | 18,846                                                                             |\n",
        "  | 카테고리 수    | 20                                                                                 |\n",
        "  | 학습·테스트 분할 | 디렉터리 기준 train/test 분리 후, 추가로 train/test 8:2 분할 사용 |\n",
        "\n",
        "- 이후 단계에서 로컬에서 로드한 texts, labels를 기준으로 정규식·불용어 제거 등 추가 전처리를 수행하고, train_test_split으로 학습/테스트 세트를 8:2로 나눈다."
      ],
      "metadata": {
        "id": "_zccicn-6FEv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3. 데이터셋·라벨 구조 및 길이 분석\n",
        "\n",
        "- 로컬에서 로드한 texts, labels를 학습용·테스트용으로 8:2 비율로 분할한다.\n",
        "\n",
        "- 이후 전처리 함수(clean_text)를 적용해 소문자 변환, 특수문자 제거, 불용어 제거를 수행하여 모델 학습에 적합한 문장 형태로 만든다."
      ],
      "metadata": {
        "id": "dekX5Htw7coK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "pGxNxGKLdJ0K"
      },
      "outputs": [],
      "source": [
        "# 학습/테스트 분할\n",
        "train_inputs, test_inputs, train_targets, test_targets = train_test_split(\n",
        "    texts, labels, test_size=0.2, random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리 함수 정의\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    tokens = text.split()\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# 전처리 적용\n",
        "train_inputs = [clean_text(text) for text in train_inputs]\n",
        "test_inputs  = [clean_text(text) for text in test_inputs]"
      ],
      "metadata": {
        "id": "8Hy1mR1D7oh3"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 전처리된 문장들을 단어 단위로 토큰화한 뒤, 각 문서의 길이(단어 수)를 측정하여 분포를 확인한다.\n",
        "\n",
        "- 히스토그램과 평균·최대값·95퍼센타일을 이용해, 패딩에 사용할 최대 길이(max_len)를 결정한다."
      ],
      "metadata": {
        "id": "dK6aN5sr_ia4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_lengths = [len(word_tokenize(msg)) for msg in train_inputs]\n",
        "test_lengths  = [len(word_tokenize(msg)) for msg in test_inputs]\n",
        "\n",
        "plt.hist(train_lengths, bins=30, alpha=0.7, label='Train Data')\n",
        "plt.hist(test_lengths, bins=30, alpha=0.7, label='Test Data')\n",
        "plt.xlabel('Message Length (words)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "plt.title('Distribution of Message Lengths')\n",
        "plt.show()\n",
        "\n",
        "print(\"훈련 데이터 평균 길이:\", np.mean(train_lengths))\n",
        "print(\"훈련 데이터 최대 길이:\", np.max(train_lengths))\n",
        "print(\"훈련 데이터 95분위수 길이:\", np.percentile(train_lengths, 95))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "id": "DLp8YwHv_mUC",
        "outputId": "0d901783-adc8-4a80-d7a7-e2463162ece4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 데이터 평균 길이: 96.30405943221014\n",
            "훈련 데이터 최대 길이: 6620\n",
            "훈련 데이터 95분위수 길이: 284.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 베이스라인에서는 95퍼센타일을 참고하여 max_len ≈ 280 정도로 설정하여, 대부분의 문서를 정보 손실 없이 커버하면서도 너무 긴 문서로 인한 연산 낭비를 줄이도록 했다."
      ],
      "metadata": {
        "id": "gkwXCdDjAFVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 280"
      ],
      "metadata": {
        "id": "05Je5yqFXsp_"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. 임베딩 기반 텍스트 분류 모델 구현"
      ],
      "metadata": {
        "id": "KdPMSDAzFAjP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1. 데이터셋 클래스 및 데이터 로더 정의\n",
        "- 목표: LSTM에 바로 넣을 수 있는 형태의 텐서로 바꿔 주는 공통 파이프라인을 만든다.  \n",
        "\\*전처리된 텍스트를 토큰화 → 단어를 정수 인덱스로 변환 → max_len까지 패딩/자르기\n",
        "\n",
        "- 이 클래스는 임베딩 종류(Word2Vec, FastText, GloVe)에 상관없이 동일하게 재사용하고, 임베딩마다 달라지는 것은 word2idx 사전과 임베딩 행렬뿐이다."
      ],
      "metadata": {
        "id": "j_3KoTpmFB-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextEmbeddingDataset(Dataset):\n",
        "    def __init__(self, texts, labels, word2idx, max_len):\n",
        "        self.texts = texts          # 전처리된 문자열 리스트\n",
        "        self.labels = labels        # 정수 레이블 리스트\n",
        "        self.word2idx = word2idx    # 단어 → 인덱스 매핑 (임베딩별로 다름)\n",
        "        self.max_len = max_len      # 패딩/자르기 기준 길이\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # 1) 문장 토큰화\n",
        "        tokens = word_tokenize(self.texts[idx])\n",
        "\n",
        "        # 2) 각 단어를 인덱스로 변환 (사전에 없으면 0으로 처리)\n",
        "        encoded = [self.word2idx.get(word, 0) for word in tokens]\n",
        "\n",
        "        # 3) max_len 기준으로 패딩 또는 자르기\n",
        "        if len(encoded) < self.max_len:\n",
        "            encoded += [0] * (self.max_len - len(encoded))\n",
        "        else:\n",
        "            encoded = encoded[:self.max_len]\n",
        "\n",
        "        # 4) 텐서로 변환 (입력: long, 레이블: long)\n",
        "        return torch.tensor(encoded, dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)"
      ],
      "metadata": {
        "id": "S5nvOBADF3e9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 이후 공통 클래스에 각 임베딩별 사전만 넣어주면 된다."
      ],
      "metadata": {
        "id": "rjK20-SzHB1J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2. 임베딩 벡터(Word2Vec, FastText, GloVe) 구성\n",
        "\n",
        "- 목표: 같은 텍스트 데이터를 세 가지 방식으로 벡터화하여, 임베딩 방식만 다르게 한 상태에서 LSTM 분류 성능을 비교한다."
      ],
      "metadata": {
        "id": "NUfB1RFkH8K4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Word2Vec\n",
        "\n",
        "- 전처리된 train_inputs를 단어 단위로 토큰화해 train_sentences를 만들고, 이를 이용해 Word2Vec을 직접 학습한다.\n",
        "\n",
        "- 학습된 모델의 단어 목록(wv.index_to_key)을 기준으로 단어→인덱스 매핑(word2idx_word2vec)을 만들고, 해당 크기에 맞는 임베딩 행렬 word2vec_matrix를 구성한다."
      ],
      "metadata": {
        "id": "UsoFNV6NMQaz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 토큰화\n",
        "train_sentences = [word_tokenize(text) for text in train_inputs]\n",
        "\n",
        "# Word2Vec 학습\n",
        "word2vec_model = Word2Vec(\n",
        "    sentences=train_sentences,\n",
        "    vector_size=128,\n",
        "    window=5,\n",
        "    min_count=1,\n",
        "    sg=1\n",
        ")\n",
        "\n",
        "# 임베딩 행렬 및 단어 사전\n",
        "word2vec_matrix = np.zeros((len(word2vec_model.wv) + 1, 128))\n",
        "word2idx_word2vec = {\n",
        "    word: idx + 1 for idx, word in enumerate(word2vec_model.wv.index_to_key)\n",
        "}\n",
        "\n",
        "for word, idx in word2idx_word2vec.items():\n",
        "    word2vec_matrix[idx] = word2vec_model.wv[word]"
      ],
      "metadata": {
        "id": "Tl0vR6CcMN0c"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### FastText\n",
        "- 동일한 train_sentences를 사용하되, FastText는 subword 정보를 활용하므로 OOV 단어에 조금 더 강인한 임베딩을 제공할 수 있다.\n",
        "\n",
        "- Word2Vec과 동일한 방식으로 word2idx_fasttext와 fasttext_matrix를 구성한다."
      ],
      "metadata": {
        "id": "LLwy6KeuMVIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fasttext_model = FastText(\n",
        "    sentences=train_sentences,\n",
        "    vector_size=128,\n",
        "    window=5,\n",
        "    min_count=1,\n",
        "    sg=1\n",
        ")\n",
        "\n",
        "fasttext_matrix = np.zeros((len(fasttext_model.wv) + 1, 128))\n",
        "word2idx_fasttext = {\n",
        "    word: idx + 1 for idx, word in enumerate(fasttext_model.wv.index_to_key)\n",
        "}\n",
        "\n",
        "for word, idx in word2idx_fasttext.items():\n",
        "    fasttext_matrix[idx] = fasttext_model.wv[word]"
      ],
      "metadata": {
        "id": "PNc1PVUiMoh6"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### GloVe\n",
        "- GloVe는 이미 대규모 말뭉치에서 학습된 사전학습 임베딩을 사용하며, 로컬에 다운로드한 glove.6B.200d.txt를 불러와 glove_embeddings 딕셔너리를 만든다.\n",
        "\n",
        "- 이 단어 목록으로 word2idx_glove, glove_matrix를 만들고, 없는 단어(OOV)는 0 벡터로 둔다."
      ],
      "metadata": {
        "id": "KPsbe_7LMVLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "\n",
        "# GloVe 다운로드 및 임베딩 로드\n",
        "GLOVE_URL = \"https://nlp.stanford.edu/data/glove.6B.zip\"\n",
        "GLOVE_ZIP = \"glove.6B.zip\"\n",
        "GLOVE_FILE = \"glove.6B.200d.txt\"\n",
        "\n",
        "if not os.path.exists(GLOVE_FILE):\n",
        "    print(\"Downloading GloVe embeddings...\")\n",
        "    urllib.request.urlretrieve(GLOVE_URL, GLOVE_ZIP)\n",
        "    import zipfile\n",
        "    with zipfile.ZipFile(GLOVE_ZIP, 'r') as zip_ref:\n",
        "        zip_ref.extractall()\n",
        "    print(\"GloVe embeddings downloaded and extracted.\")\n",
        "else:\n",
        "    print(\"GloVe embeddings already available.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qi3Byhx7YAX7",
        "outputId": "35bc6eeb-3452-4b58-faf0-815ce793e063"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading GloVe embeddings...\n",
            "GloVe embeddings downloaded and extracted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 200\n",
        "glove_embeddings = {}\n",
        "\n",
        "with open(GLOVE_FILE, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coeffs = np.asarray(values[1:], dtype='float32')\n",
        "        glove_embeddings[word] = coeffs\n",
        "\n",
        "word2idx_glove = {word: idx + 1 for idx, word in enumerate(glove_embeddings.keys())}\n",
        "glove_matrix = np.zeros((len(word2idx_glove) + 1, embedding_dim))\n",
        "\n",
        "for word, idx in word2idx_glove.items():\n",
        "    embedding_vector = glove_embeddings.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        glove_matrix[idx] = embedding_vector"
      ],
      "metadata": {
        "id": "lLuW8KtYMvWA"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 본 프로젝트에서는 동일한 LSTM 기반 분류 모델을 사용한 상태에서, 임베딩 방식(Word2Vec, FastText, GloVe)에 따른 성능 차이를 비교하는 데 초점을 둔다. 이 중 GloVe는 직접 학습 임베딩이 아니라, 대규모 코퍼스에서 사전 학습된 임베딩을 사용하기 때문에 보다 풍부한 일반 어휘 정보를 담고 있어, 뉴스 도메인 텍스트 분류에서 상대적으로 유리할 것으로 가정한다."
      ],
      "metadata": {
        "id": "0MZOYyHmKKa3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3. LSTM 기반 분류 모델 구조 정의\n",
        "\n",
        "- 목표: “단어 인덱스 시퀀스 → 임베딩 벡터 시퀀스 → LSTM → 최종 분류 점수” 흐름을 입력 관점에서 명확히 설명한다.\n",
        "\n",
        "- 입력 흐름 핵심 아이디어:앞에서 만든 임베딩 행렬(Word2Vec, FastText, GloVe 중 하나)을 Embedding 레이어의 가중치로 사용한다.\n",
        "\n",
        "- TextEmbeddingDataset이 만들어 준 정수 시퀀스(배치 크기 × max_len)를 Embedding 레이어에 넣으면, 각 인덱스가 해당 단어 벡터로 치환된다.\n",
        "\n",
        "- 이렇게 만들어진 (batch_size, max_len, embedding_dim) 텐서가 LSTM으로 전달되어, 순서 정보를 반영한 문장 표현을 만든다."
      ],
      "metadata": {
        "id": "_Hpo-frCKkbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class EmbeddingLSTM(nn.Module):\n",
        "    def __init__(self, embedding_matrix, hidden_dim, output_dim,\n",
        "                 num_layers=2, dropout=0.5):\n",
        "        super(EmbeddingLSTM, self).__init__()\n",
        "\n",
        "        # 1) 임베딩 레이어: 사전 준비한 임베딩 행렬을 가중치로 사용\n",
        "        num_embeddings, embedding_dim = embedding_matrix.shape\n",
        "\n",
        "        self.embedding = nn.Embedding.from_pretrained(\n",
        "            torch.tensor(embedding_matrix, dtype=torch.float),\n",
        "            freeze= True  # 학습 중 임베딩 업데이트 안 함(추가 실험에서 Glove 의 freeze = False 로 두어서 성능 비교 예정)\n",
        "        )\n",
        "\n",
        "        # 2) LSTM: 임베딩 벡터 시퀀스를 입력으로 받아 문맥 정보를 학습\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        # 3) 출력층: LSTM이 요약한 정보를 이용해 20개 뉴스 카테고리로 분류\n",
        "        # (양방향이므로 hidden_dim * 2 가 더 자연스럽지만,\n",
        "        #  베이스라인 구조에 맞춰 hidden_dim만 사용하는 형태도 가능)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, max_len) — 단어 인덱스 시퀀스\n",
        "        embedded = self.embedding(x)\n",
        "        # embedded: (batch_size, max_len, embedding_dim)\n",
        "\n",
        "        # LSTM 통과 (전체 시퀀스 처리)\n",
        "        _, (hidden, _) = self.lstm(embedded)\n",
        "        # hidden: (num_layers * num_directions, batch_size, hidden_dim)\n",
        "\n",
        "        # 마지막 레이어의 hidden state 사용 (문장 요약 벡터)\n",
        "        out = self.fc(hidden[-1])\n",
        "        # out: (batch_size, output_dim = 20)\n",
        "        return out"
      ],
      "metadata": {
        "id": "VxtoBFsaK4RR"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 위 구조에서 입력 흐름:\n",
        "    - Dataset이 만든 인덱스 시퀀스 → Embedding 레이어가 임베딩 행렬에서 해당 행(row)을 찾아 벡터 시퀀스로 변환 → LSTM이 이 벡터 시퀀스를 시간 순서대로 읽으며 hidden state를 쌓음 → 마지막 hidden state를 FC 레이어에 넣어 20개 카테고리 로짓을 출력.\n",
        "\n",
        "- 임베딩을 바꿀 때는 오직 embedding_matrix만 교체하면 되므로, Word2Vec / FastText / GloVe 모두 동일한 LSTM 구조를 재사용할 수 있다."
      ],
      "metadata": {
        "id": "eyuEvuFxK5Ci"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4. 학습 설정 및 모델 학습(임베딩별 실험 포함)"
      ],
      "metadata": {
        "id": "4a0gli_WNmfz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  공통 학습 설정(공정 비교를 위한 기준)\n",
        "\n",
        "- 목적: 임베딩만 다르고, 나머지 조건은 모두 동일하게 맞춰 공정 비교를 한다.\n",
        "\n",
        "- 공통으로 사용하는 설정 예시:\n",
        "    - 에폭 수: 10\n",
        "    - 배치 크기: 64 (GloVe만 32처럼 다르게 둘 수도 있지만, 가능하면 통일하는 방향 고려)\n",
        "    - 옵티마이저: Adam\n",
        "    - 학습률: 0.005\n",
        "    - 손실 함수: CrossEntropyLoss"
      ],
      "metadata": {
        "id": "thkS6A8qNoLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_dim = 128\n",
        "output_dim = len(set(labels))\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "lr = 0.005\n",
        "num_epochs = 10\n",
        "batch_size = 64"
      ],
      "metadata": {
        "id": "kjfm0VpMNzIF"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 공통 학습·평가 루프 정의\n",
        "\n",
        "- 동일한 train / evaluate 함수를 정의해, 임베딩 종류만 바꿔가며 재사용한다."
      ],
      "metadata": {
        "id": "yBd5g42yN-yl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, loader, criterion, optimizer):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for texts, labels in loader:\n",
        "        texts, labels = texts.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(texts)          # (batch_size, num_classes)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for texts, labels in loader:\n",
        "            texts, labels = texts.to(device), labels.to(device)\n",
        "            outputs = model(texts)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total   += labels.size(0)\n",
        "    return correct / total"
      ],
      "metadata": {
        "id": "S8av0wDsODda"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def get_classification_report(model, loader, target_names=None):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for texts, labels in loader:\n",
        "            texts, labels = texts.to(device), labels.to(device)\n",
        "            outputs = model(texts)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    print(classification_report(all_labels, all_preds, target_names=target_names))"
      ],
      "metadata": {
        "id": "bMXbur7JRUCz"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  임베딩별 학습 루프 (Word2Vec / FastText / GloVe)\n",
        "- 각 임베딩에 대해:\n",
        "    - 공통 TextEmbeddingDataset 클래스를 사용해 임베딩 전용 Dataset·DataLoader를 구성\n",
        "    - 해당 임베딩 행렬로 EmbeddingLSTM 생성\n",
        "    - 같은 loss, optimizer, epoch 수로 학습\n",
        "    - 동일한 evaluate로 테스트 정확도 측정"
      ],
      "metadata": {
        "id": "dCd1iIdPOEHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Word2Vec Dataset / DataLoader 생성\n",
        "\n",
        "train_dataset_word2vec = TextEmbeddingDataset(\n",
        "    texts=train_inputs,\n",
        "    labels=train_targets,\n",
        "    word2idx=word2idx_word2vec,\n",
        "    max_len=max_len\n",
        ")\n",
        "test_dataset_word2vec = TextEmbeddingDataset(\n",
        "    texts=test_inputs,\n",
        "    labels=test_targets,\n",
        "    word2idx=word2idx_word2vec,\n",
        "    max_len=max_len\n",
        ")\n",
        "\n",
        "train_loader_word2vec = DataLoader(\n",
        "    train_dataset_word2vec,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        ")\n",
        "test_loader_word2vec = DataLoader(\n",
        "    test_dataset_word2vec,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False\n",
        ")"
      ],
      "metadata": {
        "id": "oPv4nrHWZkck"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Word2Vec 학습 루프\n",
        "\n",
        "model_word2vec = EmbeddingLSTM(\n",
        "    embedding_matrix=word2vec_matrix,\n",
        "    hidden_dim=hidden_dim,\n",
        "    output_dim=output_dim\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model_word2vec.parameters(), lr=lr)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    loss = train(model_word2vec, train_loader_word2vec, loss_fn, optimizer)\n",
        "    print(f\"[Word2Vec] Epoch {epoch+1}, Loss: {loss:.4f}\")\n",
        "\n",
        "acc_word2vec = evaluate(model_word2vec, test_loader_word2vec)\n",
        "print(f\"Test Accuracy (Word2Vec + LSTM): {acc_word2vec:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHV1JhU8RmOZ",
        "outputId": "62e0aa68-9d08-44ce-f976-2b6d1f49b6b2"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Word2Vec] Epoch 1, Loss: 2.3161\n",
            "[Word2Vec] Epoch 2, Loss: 1.7203\n",
            "[Word2Vec] Epoch 3, Loss: 1.3913\n",
            "[Word2Vec] Epoch 4, Loss: 1.2326\n",
            "[Word2Vec] Epoch 5, Loss: 1.1466\n",
            "[Word2Vec] Epoch 6, Loss: 1.0597\n",
            "[Word2Vec] Epoch 7, Loss: 1.0079\n",
            "[Word2Vec] Epoch 8, Loss: 0.9514\n",
            "[Word2Vec] Epoch 9, Loss: 0.9065\n",
            "[Word2Vec] Epoch 10, Loss: 0.8544\n",
            "Test Accuracy (Word2Vec + LSTM): 0.6565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FastText Dataset / DataLoader 생성\n",
        "\n",
        "train_dataset_fasttext = TextEmbeddingDataset(\n",
        "    texts=train_inputs,\n",
        "    labels=train_targets,\n",
        "    word2idx=word2idx_fasttext,\n",
        "    max_len=max_len\n",
        ")\n",
        "test_dataset_fasttext = TextEmbeddingDataset(\n",
        "    texts=test_inputs,\n",
        "    labels=test_targets,\n",
        "    word2idx=word2idx_fasttext,\n",
        "    max_len=max_len\n",
        ")\n",
        "\n",
        "train_loader_fasttext = DataLoader(\n",
        "    train_dataset_fasttext,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        ")\n",
        "test_loader_fasttext = DataLoader(\n",
        "    test_dataset_fasttext,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False\n",
        ")"
      ],
      "metadata": {
        "id": "HhLHaLLrZx7I"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FastText 학습 루프\n",
        "\n",
        "model_fasttext = EmbeddingLSTM(\n",
        "    embedding_matrix=fasttext_matrix,\n",
        "    hidden_dim=hidden_dim,\n",
        "    output_dim=output_dim\n",
        ").to(device)\n",
        "\n",
        "optimizer_fasttext = torch.optim.Adam(model_fasttext.parameters(), lr=lr)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    loss = train(model_fasttext, train_loader_fasttext, loss_fn, optimizer_fasttext)\n",
        "    print(f\"[FastText] Epoch {epoch+1}, Loss: {loss:.4f}\")\n",
        "\n",
        "acc_fasttext = evaluate(model_fasttext, test_loader_fasttext)\n",
        "print(f\"Test Accuracy (FastText + LSTM): {acc_fasttext:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWf9D2DIRmSB",
        "outputId": "4092fda3-919a-4dfa-c57f-39417639f8d0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FastText] Epoch 1, Loss: 2.1476\n",
            "[FastText] Epoch 2, Loss: 1.6540\n",
            "[FastText] Epoch 3, Loss: 1.4059\n",
            "[FastText] Epoch 4, Loss: 1.2406\n",
            "[FastText] Epoch 5, Loss: 1.1528\n",
            "[FastText] Epoch 6, Loss: 1.0757\n",
            "[FastText] Epoch 7, Loss: 1.0201\n",
            "[FastText] Epoch 8, Loss: 0.9571\n",
            "[FastText] Epoch 9, Loss: 0.9355\n",
            "[FastText] Epoch 10, Loss: 0.8754\n",
            "Test Accuracy (FastText + LSTM): 0.6507\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GloVe Dataset / DataLoader 생성\n",
        "\n",
        "train_dataset_glove = TextEmbeddingDataset(\n",
        "    texts=train_inputs,\n",
        "    labels=train_targets,\n",
        "    word2idx=word2idx_glove,\n",
        "    max_len=max_len\n",
        ")\n",
        "test_dataset_glove = TextEmbeddingDataset(\n",
        "    texts=test_inputs,\n",
        "    labels=test_targets,\n",
        "    word2idx=word2idx_glove,\n",
        "    max_len=max_len\n",
        ")\n",
        "\n",
        "train_loader_glove = DataLoader(\n",
        "    train_dataset_glove,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        ")\n",
        "test_loader_glove = DataLoader(\n",
        "    test_dataset_glove,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False\n",
        ")"
      ],
      "metadata": {
        "id": "77A1DvZ-Z3pK"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GloVe 학습 루프\n",
        "\n",
        "model_glove = EmbeddingLSTM(\n",
        "    embedding_matrix=glove_matrix,\n",
        "    hidden_dim=hidden_dim,\n",
        "    output_dim=output_dim\n",
        ").to(device)\n",
        "\n",
        "optimizer_glove = torch.optim.Adam(model_glove.parameters(), lr=lr)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    loss = train(model_glove, train_loader_glove, loss_fn, optimizer_glove)\n",
        "    print(f\"[GloVe] Epoch {epoch+1}, Loss: {loss:.4f}\")\n",
        "\n",
        "acc_glove = evaluate(model_glove, test_loader_glove)\n",
        "print(f\"Test Accuracy (GloVe + LSTM): {acc_glove:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prR4-x0GRmVh",
        "outputId": "466068d3-0547-49e8-d219-fc7ba1f07f5b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[GloVe] Epoch 1, Loss: 1.8996\n",
            "[GloVe] Epoch 2, Loss: 1.3281\n",
            "[GloVe] Epoch 3, Loss: 1.0882\n",
            "[GloVe] Epoch 4, Loss: 0.8959\n",
            "[GloVe] Epoch 5, Loss: 0.7689\n",
            "[GloVe] Epoch 6, Loss: 0.6662\n",
            "[GloVe] Epoch 7, Loss: 0.5663\n",
            "[GloVe] Epoch 8, Loss: 0.5393\n",
            "[GloVe] Epoch 9, Loss: 0.4619\n",
            "[GloVe] Epoch 10, Loss: 0.4193\n",
            "Test Accuracy (GloVe + LSTM): 0.6854\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Word2Vec classification report\")\n",
        "get_classification_report(model_word2vec, test_loader_word2vec)\n",
        "\n",
        "print(\"FastText classification report\")\n",
        "get_classification_report(model_fasttext, test_loader_fasttext)\n",
        "\n",
        "print(\"GloVe classification report\")\n",
        "get_classification_report(model_glove, test_loader_glove)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Z5v_W8XRx0w",
        "outputId": "39816671-3d0e-432f-b1f4-761e39d2cc20"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.52      0.35      0.42       160\n",
            "           1       0.46      0.53      0.49       170\n",
            "           2       0.58      0.66      0.62       207\n",
            "           3       0.64      0.44      0.52       203\n",
            "           4       0.65      0.56      0.60       182\n",
            "           5       0.74      0.83      0.78       211\n",
            "           6       0.70      0.79      0.74       202\n",
            "           7       0.74      0.67      0.70       204\n",
            "           8       0.74      0.59      0.66       190\n",
            "           9       0.53      0.87      0.66       195\n",
            "          10       0.87      0.68      0.77       201\n",
            "          11       0.80      0.75      0.78       189\n",
            "          12       0.58      0.57      0.57       206\n",
            "          13       0.85      0.80      0.83       219\n",
            "          14       0.65      0.82      0.72       208\n",
            "          15       0.58      0.86      0.69       183\n",
            "          16       0.71      0.65      0.68       178\n",
            "          17       0.86      0.75      0.80       202\n",
            "          18       0.44      0.50      0.47       137\n",
            "          19       0.27      0.07      0.10       123\n",
            "\n",
            "    accuracy                           0.66      3770\n",
            "   macro avg       0.64      0.64      0.63      3770\n",
            "weighted avg       0.66      0.66      0.65      3770\n",
            "\n",
            "FastText classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.43      0.34      0.38       160\n",
            "           1       0.53      0.66      0.59       170\n",
            "           2       0.74      0.51      0.60       207\n",
            "           3       0.66      0.41      0.51       203\n",
            "           4       0.58      0.66      0.62       182\n",
            "           5       0.87      0.73      0.79       211\n",
            "           6       0.63      0.79      0.70       202\n",
            "           7       0.73      0.61      0.66       204\n",
            "           8       0.55      0.72      0.63       190\n",
            "           9       0.58      0.79      0.67       195\n",
            "          10       0.84      0.76      0.80       201\n",
            "          11       0.83      0.75      0.79       189\n",
            "          12       0.52      0.58      0.55       206\n",
            "          13       0.67      0.85      0.75       219\n",
            "          14       0.82      0.80      0.81       208\n",
            "          15       0.62      0.76      0.68       183\n",
            "          16       0.60      0.64      0.62       178\n",
            "          17       0.81      0.77      0.79       202\n",
            "          18       0.49      0.41      0.45       137\n",
            "          19       0.31      0.14      0.19       123\n",
            "\n",
            "    accuracy                           0.65      3770\n",
            "   macro avg       0.64      0.63      0.63      3770\n",
            "weighted avg       0.65      0.65      0.64      3770\n",
            "\n",
            "GloVe classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.52      0.49      0.51       160\n",
            "           1       0.63      0.49      0.55       170\n",
            "           2       0.54      0.65      0.59       207\n",
            "           3       0.58      0.56      0.57       203\n",
            "           4       0.63      0.63      0.63       182\n",
            "           5       0.74      0.72      0.73       211\n",
            "           6       0.74      0.69      0.71       202\n",
            "           7       0.78      0.64      0.70       204\n",
            "           8       0.62      0.72      0.67       190\n",
            "           9       0.88      0.81      0.84       195\n",
            "          10       0.87      0.85      0.86       201\n",
            "          11       0.80      0.77      0.78       189\n",
            "          12       0.56      0.66      0.60       206\n",
            "          13       0.83      0.87      0.85       219\n",
            "          14       0.74      0.82      0.78       208\n",
            "          15       0.69      0.79      0.74       183\n",
            "          16       0.70      0.57      0.63       178\n",
            "          17       0.81      0.84      0.83       202\n",
            "          18       0.54      0.63      0.58       137\n",
            "          19       0.31      0.24      0.27       123\n",
            "\n",
            "    accuracy                           0.69      3770\n",
            "   macro avg       0.68      0.67      0.67      3770\n",
            "weighted avg       0.69      0.69      0.68      3770\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. 성능 비교, 추가 실험 및 결론"
      ],
      "metadata": {
        "id": "6Kp_7dhsSktX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1. 모델별 성능 비교 분석\n",
        "\n",
        "- 동일한 LSTM 구조와 학습 설정(에폭 10, lr 0.005, batch 64, 임베딩 freeze)에서, 임베딩 방식만 Word2Vec / FastText / GloVe로 달리하여 성능을 비교했다.\n",
        "\n",
        "- Accuracy와 F1-macro, F1-weighted를 기준으로 볼 때, GloVe 임베딩을 사용한 모델이 전반적으로 가장 높은 성능을 보였으며, Word2Vec과 FastText는 서로 비슷한 수준에서 약간의 차이만을 보였다.\n",
        "\n",
        "  | 지표          | Word2Vec | FastText | GloVe  |\n",
        "  | ----------- | -------- | -------- | ------ |\n",
        "  | Accuracy    | 0.6565   | 0.6507   | 0.6854 |\n",
        "  | F1-macro    | 0.63     | 0.63     | 0.67   |\n",
        "  | F1-weighted | 0.65     | 0.64     | 0.68   |\n",
        "\n",
        "- Accuracy 기준으로 GloVe(+LSTM)가 약 0.69로 가장 높고, Word2Vec(약 0.66), FastText(약 0.65)가 그 뒤를 따른다.\n",
        "\n",
        "- F1-macro 역시 GloVe가 가장 높아, 전체 20개 클래스 전반에서 보다 균형 잡힌 분류 성능을 보였음을 시사한다.\n",
        "\n",
        "- 일부 클래스(예: label 19 등)에서 공통적으로 낮은 F1을 보이므로, 특정 소수 클래스에 대한 오분류가 성능 한계로 작용하고 있다."
      ],
      "metadata": {
        "id": "uGHXoQilSl_U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2. 개선 방안 및 추가 실험 결과\n",
        "\n",
        "#### 개선 실험 설계  \n",
        "- 이번 기본 실험 결과, 세 임베딩 모두 에폭 10에서 여전히 loss가 감소하는 추세였고, GloVe + LSTM 조합이 가장 높은 성능(Accuracy 약 0.685, F1-macro 약 0.67)을 보였다. 이를 기반으로, 다음과 같이 단계적으로 성능 향상을 시도하고자 한다.\n",
        "\n",
        "</br>\n",
        "\n",
        "- **[추가 실험 1] 에폭 15까지 학습(세 임베딩 공통, freeze 그대로)**\n",
        "\n",
        "    - 목적: 현재 10에폭에서 수렴이 끝나지 않은 것으로 보이므로, 추가 학습을 통해 각 임베딩의 최대 잠재 성능을 공정하게 비교한다.\n",
        "\n",
        "    - 설정: Word2Vec / FastText / GloVe 모두, 동일한 학습률(0.005), 배치 크기(64), 손실 함수(CrossEntropyLoss), 임베딩 freeze=True를 유지한 채 에폭 수만 10 → 15로 증가.\n",
        "\n",
        "    - 기대 효과:  \n",
        "        - GloVe의 성능이 추가 상승하거나, Word2Vec·FastText의 성능이 GloVe에 더 근접하는지 확인.  \n",
        "        - “에폭 수 부족”이 현재 성능 차이의 주요 원인인지 평가.\n",
        "\n",
        "</br>\n",
        "\n",
        "- **[추가 실험 2] GloVe 임베딩 파인튜닝(freeze=False, 에폭 15)**\n",
        "\n",
        "    - 목적: 사전학습 GloVe 임베딩을 20 Newsgroups 도메인에 맞게 미세 조정(fine-tuning) 했을 때의 성능 변화를 확인한다.\n",
        "\n",
        "    - 설정: GloVe 임베딩 행렬을 사용하되, EmbeddingLSTM과 동일 구조에서 임베딩 레이어만 freeze=False로 둔 모델을 별도로 정의한다.\n",
        "\n",
        "    - 학습 조건(에폭 15, lr 0.005, batch 64)은 1)과 동일하게 유지.\n",
        "\n",
        "    - 비교 기준: GloVe + ```freeze =True``` + 15 epoch vs GloVe + ```freeze = False``` + 15 epoch\n",
        "\n",
        "    - Accuracy, F1-macro, F1-weighted를 중심으로, 파인튜닝이 실제로 이득을 주는지 확인.\n",
        "\n",
        "</br>\n",
        "\n",
        "- **[추가 실험 3] 추가 실험 1 에서 유의미한 향상이 있을 경우: max_len 조정(280 → 350)**\n",
        "\n",
        "    - 전제: 실험에서 세 임베딩 모두 에폭 15 기준 성능이 10에폭보다 전반적으로 개선되었다고 판단될 때 진행\n",
        "\n",
        "    - 목적: 긴 문서의 뒤쪽 정보를 더 많이 활용해, 특히 길이가 긴 카테고리(정치/종교 등)에서의 분류 성능을 개선한다.\n",
        "\n",
        "    - 설정:\n",
        "        - 길이 분석 결과와 현재 max_len=280를 기준으로, max_len을 350 로 증가\n",
        "        - Word2Vec / FastText / GloVe 모두에 대해 “에폭 15 + ```max_len=350```” 설정으로 다시 학습\n",
        "\n",
        "    - 비교 기준:\n",
        "        - (에폭 15, max_len=280) vs (에폭 15, max_len=350)\n",
        "        - Accuracy/F1 관점에서, 추가로 보는 토큰이 실제로 도움을 주는지, 또는 연산량만 늘어나고 성능 이득은 거의 없는지 평가."
      ],
      "metadata": {
        "id": "KmWbh9X2jhDI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**추가실험 1**"
      ],
      "metadata": {
        "id": "XiUg6hMAlnqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#epochs 수 변경(10 → 15)\n",
        "num_epochs = 15"
      ],
      "metadata": {
        "id": "igYCjG0amPd3"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Word2Vec 학습 루프\n",
        "\n",
        "model_word2vec = EmbeddingLSTM(\n",
        "    embedding_matrix=word2vec_matrix,\n",
        "    hidden_dim=hidden_dim,\n",
        "    output_dim=output_dim\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model_word2vec.parameters(), lr=lr)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    loss = train(model_word2vec, train_loader_word2vec, loss_fn, optimizer)\n",
        "    print(f\"[Word2Vec] Epoch {epoch+1}, Loss: {loss:.4f}\")\n",
        "\n",
        "acc_word2vec = evaluate(model_word2vec, test_loader_word2vec)\n",
        "print(f\"Test Accuracy (Word2Vec + LSTM): {acc_word2vec:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lN4nSvOalkd8",
        "outputId": "062f647e-6100-49e1-e90a-12fe4d1f8120"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Word2Vec] Epoch 1, Loss: 2.3034\n",
            "[Word2Vec] Epoch 2, Loss: 1.7849\n",
            "[Word2Vec] Epoch 3, Loss: 1.4472\n",
            "[Word2Vec] Epoch 4, Loss: 1.2754\n",
            "[Word2Vec] Epoch 5, Loss: 1.1658\n",
            "[Word2Vec] Epoch 6, Loss: 1.0961\n",
            "[Word2Vec] Epoch 7, Loss: 1.0341\n",
            "[Word2Vec] Epoch 8, Loss: 0.9776\n",
            "[Word2Vec] Epoch 9, Loss: 0.9302\n",
            "[Word2Vec] Epoch 10, Loss: 0.8885\n",
            "[Word2Vec] Epoch 11, Loss: 0.8364\n",
            "[Word2Vec] Epoch 12, Loss: 0.7891\n",
            "[Word2Vec] Epoch 13, Loss: 0.7561\n",
            "[Word2Vec] Epoch 14, Loss: 0.7198\n",
            "[Word2Vec] Epoch 15, Loss: 0.6770\n",
            "Test Accuracy (Word2Vec + LSTM): 0.6615\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FastText 학습 루프\n",
        "\n",
        "model_fasttext = EmbeddingLSTM(\n",
        "    embedding_matrix=fasttext_matrix,\n",
        "    hidden_dim=hidden_dim,\n",
        "    output_dim=output_dim\n",
        ").to(device)\n",
        "\n",
        "optimizer_fasttext = torch.optim.Adam(model_fasttext.parameters(), lr=lr)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    loss = train(model_fasttext, train_loader_fasttext, loss_fn, optimizer_fasttext)\n",
        "    print(f\"[FastText] Epoch {epoch+1}, Loss: {loss:.4f}\")\n",
        "\n",
        "acc_fasttext = evaluate(model_fasttext, test_loader_fasttext)\n",
        "print(f\"Test Accuracy (FastText + LSTM): {acc_fasttext:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiJZJBtpl7Gb",
        "outputId": "9e5dce30-a29b-4f43-a3b0-15ebbc81f421"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FastText] Epoch 1, Loss: 2.1173\n",
            "[FastText] Epoch 2, Loss: 1.6073\n",
            "[FastText] Epoch 3, Loss: 1.3330\n",
            "[FastText] Epoch 4, Loss: 1.2347\n",
            "[FastText] Epoch 5, Loss: 1.1147\n",
            "[FastText] Epoch 6, Loss: 1.0470\n",
            "[FastText] Epoch 7, Loss: 0.9709\n",
            "[FastText] Epoch 8, Loss: 0.9329\n",
            "[FastText] Epoch 9, Loss: 0.8678\n",
            "[FastText] Epoch 10, Loss: 0.8214\n",
            "[FastText] Epoch 11, Loss: 0.7734\n",
            "[FastText] Epoch 12, Loss: 0.7661\n",
            "[FastText] Epoch 13, Loss: 0.6996\n",
            "[FastText] Epoch 14, Loss: 0.8342\n",
            "[FastText] Epoch 15, Loss: 0.7697\n",
            "Test Accuracy (FastText + LSTM): 0.6477\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GloVe 학습 루프\n",
        "\n",
        "model_glove = EmbeddingLSTM(\n",
        "    embedding_matrix=glove_matrix,\n",
        "    hidden_dim=hidden_dim,\n",
        "    output_dim=output_dim\n",
        ").to(device)\n",
        "\n",
        "optimizer_glove = torch.optim.Adam(model_glove.parameters(), lr=lr)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    loss = train(model_glove, train_loader_glove, loss_fn, optimizer_glove)\n",
        "    print(f\"[GloVe] Epoch {epoch+1}, Loss: {loss:.4f}\")\n",
        "\n",
        "acc_glove = evaluate(model_glove, test_loader_glove)\n",
        "print(f\"Test Accuracy (GloVe + LSTM): {acc_glove:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U56JwHjNl9fK",
        "outputId": "15349655-c463-43b3-b25b-caf2fdd2f5e0"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[GloVe] Epoch 1, Loss: 1.9329\n",
            "[GloVe] Epoch 2, Loss: 1.3435\n",
            "[GloVe] Epoch 3, Loss: 1.0970\n",
            "[GloVe] Epoch 4, Loss: 0.9411\n",
            "[GloVe] Epoch 5, Loss: 0.7985\n",
            "[GloVe] Epoch 6, Loss: 0.6686\n",
            "[GloVe] Epoch 7, Loss: 0.5839\n",
            "[GloVe] Epoch 8, Loss: 0.5170\n",
            "[GloVe] Epoch 9, Loss: 0.4405\n",
            "[GloVe] Epoch 10, Loss: 0.3968\n",
            "[GloVe] Epoch 11, Loss: 0.3320\n",
            "[GloVe] Epoch 12, Loss: 0.2993\n",
            "[GloVe] Epoch 13, Loss: 0.3081\n",
            "[GloVe] Epoch 14, Loss: 0.2769\n",
            "[GloVe] Epoch 15, Loss: 0.2268\n",
            "Test Accuracy (GloVe + LSTM): 0.6775\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Word2Vec classification report(epochs 15)\")\n",
        "get_classification_report(model_word2vec, test_loader_word2vec)\n",
        "\n",
        "print(\"FastText classification report(epochs 15)\")\n",
        "get_classification_report(model_fasttext, test_loader_fasttext)\n",
        "\n",
        "print(\"GloVe classification report(epochs 15)\")\n",
        "get_classification_report(model_glove, test_loader_glove)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLy_Zjb2o_Gg",
        "outputId": "9940bc1e-b2fe-4912-dff9-1660977b6cf7"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec classification report(epochs 15)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.43      0.52      0.47       160\n",
            "           1       0.53      0.49      0.51       170\n",
            "           2       0.55      0.62      0.58       207\n",
            "           3       0.55      0.67      0.60       203\n",
            "           4       0.63      0.50      0.56       182\n",
            "           5       0.79      0.79      0.79       211\n",
            "           6       0.66      0.79      0.72       202\n",
            "           7       0.77      0.67      0.72       204\n",
            "           8       0.72      0.66      0.69       190\n",
            "           9       0.74      0.74      0.74       195\n",
            "          10       0.76      0.83      0.80       201\n",
            "          11       0.73      0.79      0.76       189\n",
            "          12       0.72      0.46      0.56       206\n",
            "          13       0.88      0.77      0.82       219\n",
            "          14       0.74      0.84      0.79       208\n",
            "          15       0.64      0.78      0.70       183\n",
            "          16       0.59      0.63      0.61       178\n",
            "          17       0.84      0.76      0.79       202\n",
            "          18       0.39      0.47      0.43       137\n",
            "          19       0.29      0.15      0.19       123\n",
            "\n",
            "    accuracy                           0.66      3770\n",
            "   macro avg       0.65      0.64      0.64      3770\n",
            "weighted avg       0.66      0.66      0.66      3770\n",
            "\n",
            "FastText classification report(epochs 15)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.49      0.44      0.47       160\n",
            "           1       0.70      0.52      0.60       170\n",
            "           2       0.65      0.61      0.63       207\n",
            "           3       0.55      0.64      0.59       203\n",
            "           4       0.64      0.59      0.61       182\n",
            "           5       0.76      0.79      0.78       211\n",
            "           6       0.64      0.70      0.67       202\n",
            "           7       0.73      0.62      0.67       204\n",
            "           8       0.57      0.69      0.62       190\n",
            "           9       0.60      0.78      0.68       195\n",
            "          10       0.83      0.74      0.78       201\n",
            "          11       0.69      0.78      0.73       189\n",
            "          12       0.58      0.52      0.55       206\n",
            "          13       0.92      0.65      0.76       219\n",
            "          14       0.77      0.78      0.78       208\n",
            "          15       0.59      0.78      0.67       183\n",
            "          16       0.52      0.73      0.61       178\n",
            "          17       0.80      0.78      0.79       202\n",
            "          18       0.40      0.37      0.38       137\n",
            "          19       0.24      0.09      0.13       123\n",
            "\n",
            "    accuracy                           0.65      3770\n",
            "   macro avg       0.63      0.63      0.63      3770\n",
            "weighted avg       0.65      0.65      0.64      3770\n",
            "\n",
            "GloVe classification report(epochs 15)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.39      0.47       160\n",
            "           1       0.55      0.56      0.55       170\n",
            "           2       0.78      0.44      0.56       207\n",
            "           3       0.64      0.58      0.61       203\n",
            "           4       0.63      0.69      0.66       182\n",
            "           5       0.71      0.81      0.76       211\n",
            "           6       0.60      0.75      0.67       202\n",
            "           7       0.72      0.71      0.71       204\n",
            "           8       0.67      0.62      0.64       190\n",
            "           9       0.70      0.81      0.75       195\n",
            "          10       0.83      0.80      0.81       201\n",
            "          11       0.73      0.79      0.76       189\n",
            "          12       0.55      0.60      0.58       206\n",
            "          13       0.89      0.80      0.84       219\n",
            "          14       0.77      0.80      0.78       208\n",
            "          15       0.72      0.77      0.74       183\n",
            "          16       0.63      0.63      0.63       178\n",
            "          17       0.81      0.80      0.81       202\n",
            "          18       0.53      0.54      0.53       137\n",
            "          19       0.38      0.45      0.41       123\n",
            "\n",
            "    accuracy                           0.68      3770\n",
            "   macro avg       0.67      0.67      0.66      3770\n",
            "weighted avg       0.68      0.68      0.68      3770\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**추가실험 1 실행 결과**"
      ],
      "metadata": {
        "id": "4qIqDE-Xmx7Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  | 지표          | Word2Vec | FastText | GloVe  |\n",
        "  | ----------- | -------- | -------- | ------ |\n",
        "  | Accuracy    | 0.6565   | 0.6507   | 0.6854 |\n",
        "  | F1-macro    | 0.63     | 0.63     | 0.67   |\n",
        "  | F1-weighted | 0.65     | 0.64     | 0.68   |"
      ],
      "metadata": {
        "id": "fKjAHXUlsteA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| 지표          | Word2Vec (10) | Word2Vec (15) | FastText (10) | FastText (15) | GloVe (10) | GloVe (15) |\n",
        "| ----------- | ------------- | ------------- | ------------- | ------------- | ---------- | ---------- |\n",
        "| Accuracy    | 0.6565        | 0.6615        | 0.6507        | 0.6477        | 0.6854     | 0.6775     |\n",
        "| F1-macro    | 0.63          | 0.64          | 0.63          | 0.63          | 0.67       | 0.66       |\n",
        "| F1-weighted | 0.65          | 0.66          | 0.64          | 0.64          | 0.68       | 0.68       |"
      ],
      "metadata": {
        "id": "5j2MsvaOp6-S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Word2Vec\n",
        "    - Accuracy: 0.6565 → 0.6615 (소폭 상승)\n",
        "    - F1-macro: 0.63 → 0.64, F1-weighted: 0.65 → 0.66\n",
        "    - 전반적으로 약간의 성능 향상이 있었고, 특히 몇몇 클래스에서 recall·precision이 더 균형 있게 개선되었다.\n",
        "\n",
        "- FastText\n",
        "    - Accuracy: 0.6507 → 0.6477 (거의 변화 없음, 약간 하락)\n",
        "    - F1-macro/weighted도 0.63 전후로 큰 변화 없이 유지.\n",
        "    - 일부 클래스에서 변동이 있지만 전체 성능은 사실상 비슷한 수준으로 머무름.\n",
        "\n",
        "- GloVe\n",
        "    - Accuracy: 0.6854 → 0.6775 (소폭 하락)\n",
        "    - F1-macro: 0.67 → 0.66, F1-weighted: 0.68 → 0.68 (거의 비슷, 약간의 변동)\n",
        "    - 에폭을 늘렸을 때 train loss는 계속 감소했지만, 테스트 성능은 10 에폭 근처가 가장 좋았던 것으로 보이며, 이후 약간의 과적합 경향이 나타난 것으로 해석할 수 있다.\n",
        "\n",
        "- 해석\n",
        "    - Word2Vec는 에폭 15에서 약간의 추가 이득을 보았지만, 상승 폭이 크지는 않다.\n",
        "\n",
        "    - FastText는 사실상 정체 상태로, 에폭 10 이후 추가 학습이 큰 의미를 주지 못한 상황이다.\n",
        "\n",
        "    - GloVe는 이미 10 에폭 근처에서 가장 좋은 테스트 성능을 달성했고, 이후 더 학습하면서 약간의 과적합이 생긴 것으로 보인다.\n",
        "\n",
        "    - 결론적으로, 에폭 10–15 구간에서 극적인 성능 향상은 없고, GloVe(10 에폭 기준)가 여전히 전체적으로 가장 안정적이고 우수한 성능을 보인다.\n",
        "\n",
        "    - 따라서 **max_len 변경 실험은 에폭 10 기준**으로 하는 것이 합리적이다."
      ],
      "metadata": {
        "id": "a6wNQcRmqAjH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**추가실험 2**"
      ],
      "metadata": {
        "id": "hFLC8bSmlpzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GloVe 전용 파인튜닝용 클래스를 하나 더 만듭니다. (freeze = False만 다른 버전)\n",
        "\n",
        "class EmbeddingLSTMFinetune(nn.Module):\n",
        "    def __init__(self, embedding_matrix, hidden_dim, output_dim,\n",
        "                 num_layers=2, dropout=0.5):\n",
        "        super(EmbeddingLSTMFinetune, self).__init__()\n",
        "        num_embeddings, embedding_dim = embedding_matrix.shape\n",
        "\n",
        "        self.embedding = nn.Embedding.from_pretrained(\n",
        "            torch.tensor(embedding_matrix, dtype=torch.float),\n",
        "            freeze=False  # 파인튜닝: 임베딩까지 업데이트\n",
        "        )\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout,\n",
        "            bidirectional=True\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        _, (hidden, _) = self.lstm(embedded)\n",
        "        out = self.fc(hidden[-1])\n",
        "        return out"
      ],
      "metadata": {
        "id": "r01YysJHm7TP"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GloVe + freeze = False 학습 (epoch 15 기준)\n",
        "\n",
        "model_glove_ft = EmbeddingLSTMFinetune(\n",
        "    embedding_matrix=glove_matrix,\n",
        "    hidden_dim=hidden_dim,\n",
        "    output_dim=output_dim\n",
        ").to(device)\n",
        "\n",
        "optimizer_glove_ft = torch.optim.Adam(model_glove_ft.parameters(), lr=lr)\n",
        "\n",
        "for epoch in range(num_epochs):  # num_epochs=15\n",
        "    loss = train(model_glove_ft, train_loader_glove, loss_fn, optimizer_glove_ft)\n",
        "    print(f\"[GloVe-Finetune] Epoch {epoch+1}, Loss: {loss:.4f}\")\n",
        "\n",
        "acc_glove_ft = evaluate(model_glove_ft, test_loader_glove)\n",
        "print(f\"Test Accuracy (GloVe + LSTM, freeze=False): {acc_glove_ft:.4f}\")\n",
        "\n",
        "print(\"GloVe-Finetune classification report\")\n",
        "get_classification_report(model_glove_ft, test_loader_glove)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_ebT9YPm8ZF",
        "outputId": "20425eb5-3905-4afa-a876-97d155649917"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[GloVe-Finetune] Epoch 1, Loss: 1.8144\n",
            "[GloVe-Finetune] Epoch 2, Loss: 1.0231\n",
            "[GloVe-Finetune] Epoch 3, Loss: 0.6420\n",
            "[GloVe-Finetune] Epoch 4, Loss: 0.4233\n",
            "[GloVe-Finetune] Epoch 5, Loss: 0.2861\n",
            "[GloVe-Finetune] Epoch 6, Loss: 0.2067\n",
            "[GloVe-Finetune] Epoch 7, Loss: 0.1378\n",
            "[GloVe-Finetune] Epoch 8, Loss: 0.0941\n",
            "[GloVe-Finetune] Epoch 9, Loss: 0.0737\n",
            "[GloVe-Finetune] Epoch 10, Loss: 0.0719\n",
            "[GloVe-Finetune] Epoch 11, Loss: 0.0593\n",
            "[GloVe-Finetune] Epoch 12, Loss: 0.0548\n",
            "[GloVe-Finetune] Epoch 13, Loss: 0.0651\n",
            "[GloVe-Finetune] Epoch 14, Loss: 0.0523\n",
            "[GloVe-Finetune] Epoch 15, Loss: 0.0721\n",
            "Test Accuracy (GloVe + LSTM, freeze=False): 0.6822\n",
            "GloVe-Finetune classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.52      0.49      0.51       160\n",
            "           1       0.48      0.63      0.55       170\n",
            "           2       0.60      0.64      0.62       207\n",
            "           3       0.64      0.56      0.60       203\n",
            "           4       0.58      0.75      0.66       182\n",
            "           5       0.77      0.66      0.71       211\n",
            "           6       0.74      0.66      0.70       202\n",
            "           7       0.69      0.75      0.72       204\n",
            "           8       0.64      0.69      0.66       190\n",
            "           9       0.91      0.74      0.82       195\n",
            "          10       0.89      0.80      0.84       201\n",
            "          11       0.80      0.72      0.76       189\n",
            "          12       0.61      0.66      0.63       206\n",
            "          13       0.82      0.83      0.82       219\n",
            "          14       0.80      0.73      0.76       208\n",
            "          15       0.82      0.65      0.72       183\n",
            "          16       0.62      0.62      0.62       178\n",
            "          17       0.82      0.80      0.81       202\n",
            "          18       0.50      0.60      0.54       137\n",
            "          19       0.42      0.47      0.44       123\n",
            "\n",
            "    accuracy                           0.68      3770\n",
            "   macro avg       0.68      0.67      0.68      3770\n",
            "weighted avg       0.70      0.68      0.69      3770\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**추가 실험 2 실행 결과**"
      ],
      "metadata": {
        "id": "6YJheOOvus46"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| 모델                     | Epoch | freeze | Accuracy | F1-macro | F1-weighted |\n",
        "| ---------------------- | ----- | ------ | -------- | -------- | ----------- |\n",
        "| GloVe 기본               | 10    | True   | 0.6854   | 0.67     | 0.68        |\n",
        "| GloVe (추가실험1)          | 15    | True   | 0.6775   | 0.66     | 0.68        |\n",
        "| GloVe-Finetune (추가실험2) | 15    | False  | 0.6822   | 0.68     | 0.69        |\n",
        "\n",
        "</br>\n",
        "\n",
        "- 해석\n",
        "    - 단순히 에폭만 15로 늘리고 freeze=True를 유지했을 때는, 기본 10에폭보다 오히려 테스트 Accuracy가 약간 떨어져 과적합 경향을 보였다.\n",
        "\n",
        "    - 반면, 같은 15에폭에서 freeze=False로 파인튜닝한 모델은 Accuracy는 10에폭 기본 GloVe(0.6854)와 거의 비슷한 수준(0.6822), F1-macro/weighted는 오히려 약간 더 높은 값을 보여, 클래스 전반의 균형 측면에서는 소폭 개선이 있었다고 볼 수 있다.\n",
        "\n",
        "    - 특히 macro avg가 0.68로 가장 높아진 점을 보면, 파인튜닝이 일부 클래스에서 recall/precision 균형을 조금 더 맞춰주는 효과를 낸 것으로 해석할 수 있다.\n",
        "\n",
        "- 정리:\n",
        "    - GloVe + freeze=True, epoch 10: 여전히 가장 간단하면서도 강력한 “기본 베이스라인”\n",
        "\n",
        "    - GloVe + freeze=False, epoch 15: Accuracy는 거의 비슷하지만, macro/weighted F1 기준으로는 약간 더 나은 “강화 버전” 후보"
      ],
      "metadata": {
        "id": "hTj2THuVuNRz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**추가실험 3**"
      ],
      "metadata": {
        "id": "BjfylDp1lp2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# max_len 280 → 350로 변경\n",
        "max_len = 350\n",
        "\n",
        "#epochs 수 변경(15 → 10) *추가 실험 1 결과, epochs 15 에서 유의미한 변화 없으므로 10으로 진행\n",
        "num_epochs = 10"
      ],
      "metadata": {
        "id": "DG_qI2DVnKre"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# max_len=350 버전 Word2Vec Dataset / DataLoader\n",
        "\n",
        "train_dataset_word2vec_350 = TextEmbeddingDataset(\n",
        "    texts=train_inputs,\n",
        "    labels=train_targets,\n",
        "    word2idx=word2idx_word2vec,\n",
        "    max_len=max_len  # 350\n",
        ")\n",
        "test_dataset_word2vec_350 = TextEmbeddingDataset(\n",
        "    texts=test_inputs,\n",
        "    labels=test_targets,\n",
        "    word2idx=word2idx_word2vec,\n",
        "    max_len=max_len\n",
        ")\n",
        "\n",
        "train_loader_word2vec_350 = DataLoader(\n",
        "    train_dataset_word2vec_350,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        ")\n",
        "test_loader_word2vec_350 = DataLoader(\n",
        "    test_dataset_word2vec_350,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# FastText max_len=350\n",
        "train_dataset_fasttext_350 = TextEmbeddingDataset(\n",
        "    train_inputs, train_targets, word2idx_fasttext, max_len\n",
        ")\n",
        "test_dataset_fasttext_350 = TextEmbeddingDataset(\n",
        "    test_inputs, test_targets, word2idx_fasttext, max_len\n",
        ")\n",
        "\n",
        "train_loader_fasttext_350 = DataLoader(train_dataset_fasttext_350, batch_size=batch_size, shuffle=True)\n",
        "test_loader_fasttext_350 = DataLoader(test_dataset_fasttext_350, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# GloVe max_len=350\n",
        "train_dataset_glove_350 = TextEmbeddingDataset(\n",
        "    train_inputs, train_targets, word2idx_glove, max_len\n",
        ")\n",
        "test_dataset_glove_350 = TextEmbeddingDataset(\n",
        "    test_inputs, test_targets, word2idx_glove, max_len\n",
        ")\n",
        "\n",
        "train_loader_glove_350 = DataLoader(train_dataset_glove_350, batch_size=batch_size, shuffle=True)\n",
        "test_loader_glove_350 = DataLoader(test_dataset_glove_350, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "9f9WNB3MnQbp"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Word2Vec + max_len=350, epoch=10\n",
        "model_word2vec_350 = EmbeddingLSTM(\n",
        "    embedding_matrix=word2vec_matrix,\n",
        "    hidden_dim=hidden_dim,\n",
        "    output_dim=output_dim\n",
        ").to(device)\n",
        "\n",
        "optimizer_w2v_350 = torch.optim.Adam(model_word2vec_350.parameters(), lr=lr)\n",
        "\n",
        "for epoch in range(num_epochs):  # num_epochs = 10\n",
        "    loss = train(model_word2vec_350, train_loader_word2vec_350, loss_fn, optimizer_w2v_350)\n",
        "    print(f\"[Word2Vec-350] Epoch {epoch+1}, Loss: {loss:.4f}\")\n",
        "\n",
        "acc_word2vec_350 = evaluate(model_word2vec_350, test_loader_word2vec_350)\n",
        "print(f\"Test Accuracy (Word2Vec + LSTM, max_len=350): {acc_word2vec_350:.4f}\")\n",
        "\n",
        "print(\"Word2Vec-350 classification report\")\n",
        "get_classification_report(model_word2vec_350, test_loader_word2vec_350)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxLIn2gWnmCN",
        "outputId": "c3362b36-8719-411d-80e0-5bc6ed9aaf08"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Word2Vec-350] Epoch 1, Loss: 2.1881\n",
            "[Word2Vec-350] Epoch 2, Loss: 1.5748\n",
            "[Word2Vec-350] Epoch 3, Loss: 1.3290\n",
            "[Word2Vec-350] Epoch 4, Loss: 1.2184\n",
            "[Word2Vec-350] Epoch 5, Loss: 1.1255\n",
            "[Word2Vec-350] Epoch 6, Loss: 1.0610\n",
            "[Word2Vec-350] Epoch 7, Loss: 0.9921\n",
            "[Word2Vec-350] Epoch 8, Loss: 0.9475\n",
            "[Word2Vec-350] Epoch 9, Loss: 0.8781\n",
            "[Word2Vec-350] Epoch 10, Loss: 0.8349\n",
            "Test Accuracy (Word2Vec + LSTM, max_len=350): 0.6684\n",
            "Word2Vec-350 classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.46      0.42      0.44       160\n",
            "           1       0.63      0.48      0.54       170\n",
            "           2       0.62      0.61      0.62       207\n",
            "           3       0.65      0.58      0.61       203\n",
            "           4       0.56      0.71      0.63       182\n",
            "           5       0.78      0.81      0.79       211\n",
            "           6       0.80      0.75      0.77       202\n",
            "           7       0.77      0.67      0.72       204\n",
            "           8       0.54      0.70      0.61       190\n",
            "           9       0.66      0.72      0.69       195\n",
            "          10       0.76      0.84      0.80       201\n",
            "          11       0.83      0.70      0.76       189\n",
            "          12       0.64      0.57      0.60       206\n",
            "          13       0.81      0.84      0.82       219\n",
            "          14       0.92      0.73      0.82       208\n",
            "          15       0.64      0.78      0.70       183\n",
            "          16       0.48      0.75      0.59       178\n",
            "          17       0.89      0.75      0.81       202\n",
            "          18       0.47      0.45      0.46       137\n",
            "          19       0.32      0.20      0.24       123\n",
            "\n",
            "    accuracy                           0.67      3770\n",
            "   macro avg       0.66      0.65      0.65      3770\n",
            "weighted avg       0.68      0.67      0.67      3770\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FastText + max_len=350, epoch=10\n",
        "\n",
        "model_fasttext_350 = EmbeddingLSTM(\n",
        "    embedding_matrix=fasttext_matrix,\n",
        "    hidden_dim=hidden_dim,\n",
        "    output_dim=output_dim\n",
        ").to(device)\n",
        "\n",
        "optimizer_ft_350 = torch.optim.Adam(model_fasttext_350.parameters(), lr=lr)\n",
        "\n",
        "for epoch in range(num_epochs):  # 10\n",
        "    loss = train(model_fasttext_350, train_loader_fasttext_350, loss_fn, optimizer_ft_350)\n",
        "    print(f\"[FastText-350] Epoch {epoch+1}, Loss: {loss:.4f}\")\n",
        "\n",
        "acc_fasttext_350 = evaluate(model_fasttext_350, test_loader_fasttext_350)\n",
        "print(f\"Test Accuracy (FastText + LSTM, max_len=350): {acc_fasttext_350:.4f}\")\n",
        "\n",
        "print(\"FastText-350 classification report\")\n",
        "get_classification_report(model_fasttext_350, test_loader_fasttext_350)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fj2r3qY5noYO",
        "outputId": "b03568f2-2972-42af-c8aa-1e7014e32648"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FastText-350] Epoch 1, Loss: 2.3132\n",
            "[FastText-350] Epoch 2, Loss: 1.7805\n",
            "[FastText-350] Epoch 3, Loss: 1.4687\n",
            "[FastText-350] Epoch 4, Loss: 1.3107\n",
            "[FastText-350] Epoch 5, Loss: 1.2235\n",
            "[FastText-350] Epoch 6, Loss: 1.1270\n",
            "[FastText-350] Epoch 7, Loss: 1.0614\n",
            "[FastText-350] Epoch 8, Loss: 1.0090\n",
            "[FastText-350] Epoch 9, Loss: 0.9615\n",
            "[FastText-350] Epoch 10, Loss: 0.9113\n",
            "Test Accuracy (FastText + LSTM, max_len=350): 0.6496\n",
            "FastText-350 classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.38      0.44       160\n",
            "           1       0.60      0.55      0.58       170\n",
            "           2       0.61      0.58      0.60       207\n",
            "           3       0.63      0.48      0.54       203\n",
            "           4       0.64      0.65      0.64       182\n",
            "           5       0.85      0.74      0.79       211\n",
            "           6       0.81      0.74      0.77       202\n",
            "           7       0.70      0.60      0.65       204\n",
            "           8       0.54      0.63      0.58       190\n",
            "           9       0.53      0.81      0.64       195\n",
            "          10       0.87      0.59      0.70       201\n",
            "          11       0.78      0.77      0.78       189\n",
            "          12       0.50      0.67      0.57       206\n",
            "          13       0.76      0.84      0.80       219\n",
            "          14       0.69      0.81      0.75       208\n",
            "          15       0.59      0.84      0.69       183\n",
            "          16       0.68      0.61      0.64       178\n",
            "          17       0.84      0.76      0.79       202\n",
            "          18       0.41      0.53      0.46       137\n",
            "          19       0.25      0.08      0.12       123\n",
            "\n",
            "    accuracy                           0.65      3770\n",
            "   macro avg       0.64      0.63      0.63      3770\n",
            "weighted avg       0.66      0.65      0.64      3770\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GloVe + max_len=350, epoch=10 (freeze=True 버전)\n",
        "\n",
        "model_glove_350 = EmbeddingLSTM(\n",
        "    embedding_matrix=glove_matrix,\n",
        "    hidden_dim=hidden_dim,\n",
        "    output_dim=output_dim\n",
        ").to(device)\n",
        "\n",
        "optimizer_glove_350 = torch.optim.Adam(model_glove_350.parameters(), lr=lr)\n",
        "\n",
        "for epoch in range(num_epochs):  # 10\n",
        "    loss = train(model_glove_350, train_loader_glove_350, loss_fn, optimizer_glove_350)\n",
        "    print(f\"[GloVe-350] Epoch {epoch+1}, Loss: {loss:.4f}\")\n",
        "\n",
        "acc_glove_350 = evaluate(model_glove_350, test_loader_glove_350)\n",
        "print(f\"Test Accuracy (GloVe + LSTM, max_len=350): {acc_glove_350:.4f}\")\n",
        "\n",
        "print(\"GloVe-350 classification report\")\n",
        "get_classification_report(model_glove_350, test_loader_glove_350)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsuUBgtgnsC5",
        "outputId": "4ed96945-5093-479a-bbf0-6a2cbce7116c"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[GloVe-350] Epoch 1, Loss: 1.9213\n",
            "[GloVe-350] Epoch 2, Loss: 1.3589\n",
            "[GloVe-350] Epoch 3, Loss: 1.1308\n",
            "[GloVe-350] Epoch 4, Loss: 0.9635\n",
            "[GloVe-350] Epoch 5, Loss: 0.8172\n",
            "[GloVe-350] Epoch 6, Loss: 0.7021\n",
            "[GloVe-350] Epoch 7, Loss: 0.6063\n",
            "[GloVe-350] Epoch 8, Loss: 0.5374\n",
            "[GloVe-350] Epoch 9, Loss: 0.4610\n",
            "[GloVe-350] Epoch 10, Loss: 0.4356\n",
            "Test Accuracy (GloVe + LSTM, max_len=350): 0.6793\n",
            "GloVe-350 classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.46      0.43      0.45       160\n",
            "           1       0.60      0.56      0.58       170\n",
            "           2       0.62      0.54      0.58       207\n",
            "           3       0.62      0.63      0.62       203\n",
            "           4       0.61      0.67      0.64       182\n",
            "           5       0.73      0.73      0.73       211\n",
            "           6       0.63      0.71      0.67       202\n",
            "           7       0.69      0.73      0.71       204\n",
            "           8       0.62      0.70      0.66       190\n",
            "           9       0.83      0.82      0.83       195\n",
            "          10       0.82      0.84      0.83       201\n",
            "          11       0.84      0.76      0.80       189\n",
            "          12       0.57      0.62      0.59       206\n",
            "          13       0.85      0.87      0.86       219\n",
            "          14       0.80      0.78      0.79       208\n",
            "          15       0.74      0.63      0.68       183\n",
            "          16       0.66      0.65      0.65       178\n",
            "          17       0.86      0.79      0.82       202\n",
            "          18       0.54      0.47      0.51       137\n",
            "          19       0.32      0.41      0.36       123\n",
            "\n",
            "    accuracy                           0.68      3770\n",
            "   macro avg       0.67      0.67      0.67      3770\n",
            "weighted avg       0.68      0.68      0.68      3770\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**추가 실험 3 실행 결과**\n"
      ],
      "metadata": {
        "id": "VJr-9rDQvPO6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- max_len 280 vs 350 성능 비교\n",
        "\n",
        "  | 지표          | Word2Vec (280) | Word2Vec (350) | FastText (280) | FastText (350) | GloVe (280) | GloVe (350) |\n",
        "  | ----------- | -------------- | -------------- | -------------- | -------------- | ----------- | ----------- |\n",
        "  | Accuracy    | 0.6565         | 0.6684         | 0.6507         | 0.6496         | 0.6854      | 0.6793      |\n",
        "  | F1-macro    | 0.63           | 0.65           | 0.63           | 0.63           | 0.67        | 0.67        |\n",
        "  | F1-weighted | 0.65           | 0.67           | 0.64           | 0.64           | 0.68        | 0.68        |\n",
        "\n",
        "</br>\n",
        "\n",
        "- Word2Vec\n",
        "    - 기존: Accuracy 0.6565, F1-macro 0.63, F1-weighted 0.65\n",
        "    - max_len=350: Accuracy 0.6684, F1-macro 0.65, F1-weighted 0.67  \n",
        "    → 정확도와 F1이 모두 소폭 상승. 문서 뒤쪽 정보를 더 본 것이 일부 클래스(특히 4, 8, 16 등)에서 recall·precision 개선에 기여한 것으로 보임.\n",
        "\n",
        "- FastText\n",
        "    - 기존: Accuracy 0.6507, F1-macro 0.63, F1-weighted 0.64\n",
        "    - max_len=350: Accuracy 0.6496, F1-macro 0.63, F1-weighted 0.64    \n",
        "    → 거의 동일 수준. max_len 증가가 FastText 모델에는 뚜렷한 이득/손해를 주지 않은 것으로 해석 가능.\n",
        "\n",
        "- GloVe\n",
        "    - 기존: Accuracy 0.6854, F1-macro 0.67, F1-weighted 0.68\n",
        "    - max_len=350: Accuracy 0.6793, F1-macro 0.67, F1-weighted 0.68      \n",
        "    → Accuracy는 약간 감소, F1-macro/weighted는 거의 동일. 긴 문서 정보를 조금 더 본다고 해서 전체 성능이 의미 있게 개선되지는 않았고, GloVe는 max_len=280 설정이 이미 적절한 타협점으로 보인다.\n",
        "\n",
        "- 요약\n",
        "    - Word2Vec는 max_len 증가에 다소 긍정적인 반응(소폭 향상)\n",
        "    - FastText는 변화 거의 없음.\n",
        "    - GloVe는 Accuracy 기준으로는 280이 살짝 더 좋고, F1 기준으론 거의 차이 없음."
      ],
      "metadata": {
        "id": "Ks2YdSOHvUvJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3. 결론 및 한계점·향후 과제"
      ],
      "metadata": {
        "id": "9gAQeBsIvPRq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**결론**\n",
        "\n",
        "1. 임베딩별 기본 성능 비교\n",
        "\n",
        "- 동일한 LSTM 구조, epoch=10, max_len=280, freeze=True 조건에서 ```GloVe + LSTM 조합```이 Accuracy와 F1 기준으로 가장 우수한 성능(Acc≈0.685, F1-macro≈0.67)을 보였다.\n",
        "\n",
        "- Word2Vec와 FastText는 비슷한 수준(Acc≈0.65~0.66, F1-macro≈0.63)으로, 임베딩 방식에 따라 소폭 차이는 있지만 극적인 성능 격차는 아니었다.\n",
        "\n",
        "</br>\n",
        "\n",
        "2. [추가 실험1] 에폭 수 증가의 영향\n",
        "- epoch 10 → 15로 늘렸을 때,\n",
        "    - Word2Vec: 소폭 개선(Acc 약간↑, F1 약간↑).\n",
        "    - FastText: 거의 변화 없음.\n",
        "    - GloVe: 오히려 Accuracy가 약간 하락해, 10 에폭 근처가 테스트 성능 면에서 더 적절한 지점이었다.  \n",
        "    → 에폭 증가만으로 큰 성능 향상은 없었고, 특히 GloVe는 10 에폭이 “적당한 수렴 지점”으로 보인다.\n",
        "\n",
        "</br>\n",
        "\n",
        "3. [추가 실험 2] GloVe 파인튜닝의 효과\n",
        "\n",
        "- GloVe + freeze=False, epoch 15 실험에서\n",
        "\n",
        "- Accuracy는 기본 GloVe(10 epoch, freeze=True)와 비슷한 수준(≈0.68),\n",
        "\n",
        "- F1-macro/weighted는 약간 더 높은 값(≈0.68/0.69)을 보여, 클래스 전반의 균형 측면에서 소폭 개선이 있었다.\n",
        "\n",
        "    → 사전학습 GloVe를 파인튜닝하는 전략은 성능 미세 향상에는 의미가 있지만, 기본 GloVe 대비 큰 점프를 주는 수준은 아니었다.\n",
        "\n",
        "</br>\n",
        "\n",
        "4. [추가 실험3] max_len 조정의 영향\n",
        "- max_len 280 → 350 실험에서\n",
        "    - Word2Vec는 약간의 이득(Acc, F1 모두 소폭 상승).\n",
        "    - FastText는 거의 변화 없음.\n",
        "    - GloVe는 Accuracy 약간 하락, F1은 거의 동일.\n",
        "\n",
        "    → 전체적으로 볼 때, max_len=280 설정은 GloVe 기준으로 이미 좋은 타협점이었고, 더 늘린다고 성능이 분명히 좋아지지는 않았다.\n",
        "\n",
        "</br>\n",
        "\n",
        "5. 결론\n",
        "\n",
        "- 최종적으로, 이번 미션에서 “가장 균형 잡힌 기본 모델”로는 ```GloVe + LSTM, epoch=10, max_len=280, freeze=True``` 를 선정할 수 있다.\n",
        "\n",
        "- 추가로 성능을 약간 더 끌어올리고 싶다면 ```GloVe + LSTM, epoch≈15, freeze=False(파인튜닝)``` 를 보완 실험 결과로 제시하는 구성이 적절하다."
      ],
      "metadata": {
        "id": "AVsc8c10vW5o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  **한계점**\n",
        "\n",
        "1. 별도 validation set과 val loss 모니터링 부재\n",
        "\n",
        "- 이번 실험에서는 train/test만 사용하고, 중간에 validation loss를 별도로 모니터링하지 않았기 때문에 적절한 epoch를 조기에 판단하거나, Early Stopping을 적용해 최적 지점을 자동으로 찾는 과정이 부족했다.\n",
        "\n",
        "- 그 결과, GloVe의 경우 epoch 10 근처에서 이미 최적에 도달했음에도, epoch 15까지 늘리면서 약간의 과적합이 발생하는 패턴을 뒤늦게 확인하게 되었다.\n",
        "</br>\n",
        "\n",
        "2. 하이퍼파라미터 탐색 범위 제한\n",
        "\n",
        "- 학습률(lr=0.005), hidden_dim(128), LSTM layer 수(2) 등은 고정된 값으로 사용했고, 체계적인 하이퍼파라미터 탐색(grid/random search 등)을 수행하지 못했다.\n",
        "\n",
        "- 따라서 현재 best 설정이 “전역적으로 최적”이라고 보긴 어렵고, 더 나은 조합이 존재할 가능성이 있다.\n",
        "\n",
        "</br>\n",
        "\n",
        "3. 전처리·모델 구조 변화는 제한적으로만 탐색\n",
        "\n",
        "- 전처리는 기본적인 소문자 변환, 특수문자 제거, 불용어 제거 위주였고, lemmatization, n-gram, 도메인 특화 토큰 처리 등은 다루지 않았다.\n",
        "\n",
        "- 모델 구조도 Bi-LSTM + FC 한 가지 형태에 집중했으며, GRU, CNN+RNN, attention 등은 비교하지 못했다."
      ],
      "metadata": {
        "id": "yn13huXPu3am"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**향후 과제**\n",
        "\n",
        "1. 명시적인 validation set 구성 + Early Stopping\n",
        "\n",
        "- `train/val/test`를 6:2:2 또는 7:1:2와 같이 나누고, val loss/accuracy를 매 epoch 모니터링하며 `Early Stopping`을 도입하면 GloVe처럼 10 epoch 근처에서 최적이 나오는 모델을 더 효율적으로 학습시킬 수 있다.\n",
        "\n",
        "</br>\n",
        "\n",
        "2. 하이퍼파라미터 및 구조 확장 실험\n",
        "\n",
        "- `hidden_dim`, `LSTM layer 수`, `dropout`, `learning rate`에 대해 소규모 grid search나 random search를 수행해, “GloVe + LSTM” 구조 내에서 더 나은 조합을 찾을 수 있다.\n",
        "\n",
        "- `Bi-GRU`, `CNN+LSTM`, `attention layer 추`가 등 구조적인 변형도 비교 대상에 포함하면, 임베딩 외의 모델 구조가 성능에 미치는 영향도 함께 분석할 수 있다.\n",
        "\n",
        "</br>\n",
        "\n",
        "3. 전처리·토큰화 개선\n",
        "\n",
        "- `lemmatization / stemming`, 더 정교한 불용어 리스트, 숫자/URL/특수 패턴 처리 등\n",
        "\n",
        "- 전처리 전략을 다양화해 비교하면, 특히 의미가 분산된 클래스(예: label 19 등) 성능을 개선할 여지가 있다.\n",
        "\n",
        "- `서브워드 기반 토크나이저(BPE 등)`와 FastText를 결합해, 희귀 단어·오타에 대한 강인성을 높이는 실험도 고려할 수 있다.\n",
        "\n",
        "</br>\n",
        "\n",
        "4. 임베딩 확장 및 다른 사전학습 모델 비교\n",
        "\n",
        "- 현재는 Word2Vec / FastText / GloVe에 국한되었지만, 더 `최신 사전학습 언어모델(BERT 계열 등)`의 문장 임베딩을 사용해 같은 분류기를 학습시켜, “고전적 단어 임베딩 vs 문장 단위 딥 임베딩” 간 성능 차이도 장기적으로 비교할 수 있다."
      ],
      "metadata": {
        "id": "ZKKOiMcCvZjT"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}